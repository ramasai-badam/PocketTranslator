import * as FileSystem from 'expo-file-system';
import { initLlama } from 'llama.rn';
import { initWhisper } from 'whisper.rn';
import { getModelPath } from './ModelConfig';

// Global model contexts to persist across component unmounts
let llamaContext: any = null;
let whisperContext: any = null;
let isLlamaInitializing = false;
let isWhisperInitializing = false;

// Streaming functionality
let isStreamingEnabled = true; // Re-enable streaming now that basic translation works
const streamingListeners = new Set<(chunk: string, isComplete: boolean) => void>();

// Event listeners for model state changes
const listeners = new Set<() => void>();

// Queue for linguistic analysis requests to prevent "Context is busy" errors
let analysisQueue: Array<{
  resolve: (value: any) => void;
  reject: (error: any) => void;
  params: {
    originalText: string;
    fromLanguage: string;
    translatedText: string;
    toLanguage: string;
  };
}> = [];
let isProcessingAnalysis = false;

const notifyListeners = () => {
  listeners.forEach(listener => listener());
};

export class ModelManager {
  private static readonly MODEL_PATHS = {
    whisper: getModelPath('whisper'),
    llama: getModelPath('llama')
  };

  // Global model management methods
  static addListener(listener: () => void): () => void {
    listeners.add(listener);
    return () => listeners.delete(listener);
  }

  static isLlamaReady(): boolean {
    return llamaContext !== null;
  }

  static isWhisperReady(): boolean {
    return whisperContext !== null;
  }

  static isLlamaInitializing(): boolean {
    return isLlamaInitializing;
  }

  static isWhisperInitializing(): boolean {
    return isWhisperInitializing;
  }

  static getLlamaContext(): any {
    return llamaContext;
  }

  // Public method to check if any analysis is currently in progress
  static isAnalysisInProgress(): boolean {
    return isProcessingAnalysis;
  }

  static getWhisperContext(): any {
    return whisperContext;
  }

  // Streaming functionality methods
  static enableStreaming(enabled: boolean = true): void {
    isStreamingEnabled = enabled;
    console.log(`üåä Streaming ${enabled ? 'ENABLED' : 'DISABLED'}`);
  }

  static isStreamingEnabled(): boolean {
    return isStreamingEnabled;
  }

  static addStreamingListener(listener: (chunk: string, isComplete: boolean) => void): () => void {
    streamingListeners.add(listener);
    return () => streamingListeners.delete(listener);
  }

  static notifyStreamingListeners(chunk: string, isComplete: boolean = false): void {
    streamingListeners.forEach(listener => listener(chunk, isComplete));
  }

  // Main translation method with streaming support
  static async translateText(text: string, fromLang: string, toLang: string): Promise<string> {
    if (!llamaContext) {
      throw new Error('Llama context not initialized');
    }

    const startTime = Date.now();

    try {
      // Simple, direct prompt using the specified format
      const prompt = `<start_of_turn>user
Translate this ${fromLang} text to ${toLang}: "${text}" Strictly Provide only single translation.
<end_of_turn>
<start_of_turn>model
`;

      console.log(`üöÄ Sending prompt: ${prompt.length} characters`);

      let fullTranslation = '';

      if (isStreamingEnabled) {
        
        let hasRealStreaming = false;
        let streamingBuffer = '';
        
        // Use the correct streaming callback as per documentation
        // completion(params, callback?) - callback is the second parameter
        const result = await llamaContext.completion({
          prompt,
          n_predict: 64,
          temperature: 0.0,
          top_p: 0.1,
          top_k: 1,
          stop: ['<end_of_turn>', '<start_of_turn>'],
          seed: 42,
        }, (data: any) => {
          // This is the real streaming callback!
          hasRealStreaming = true;
          console.log('üåä Real streaming data received:', data);
          
          // Handle different possible data structures from TokenData
          let token = '';
          if (typeof data === 'string') {
            token = data;
          } else if (data && data.token) {
            token = data.token;
          } else if (data && data.text) {
            token = data.text;
          } else if (data && data.content) {
            token = data.content;
          }
          
          if (token) {
            streamingBuffer += token;
            console.log(`üåä Real token: "${token}" | Buffer: "${streamingBuffer}"`);
            this.notifyStreamingListeners(token, false);
          }
        });

        // If real streaming worked, use the buffer, otherwise use result
        if (hasRealStreaming && streamingBuffer) {
          fullTranslation = streamingBuffer.trim();
          console.log('üåä Using real streaming buffer:', fullTranslation);
        } else {
          fullTranslation = result.text?.trim() || '';
          console.log('üåä Real streaming failed, using completion result:', fullTranslation);
          
          // If we have a result but no streaming, simulate it for UX
          if (fullTranslation && !hasRealStreaming) {
            console.log('üåä Simulating streaming with real result...');
            for (let i = 0; i < fullTranslation.length; i++) {
              const char = fullTranslation[i];
              console.log(`üåä Simulated char: "${char}"`);
              this.notifyStreamingListeners(char, false);
              // Faster simulation since it's fallback
              await new Promise(resolve => setTimeout(resolve, 30));
            }
          }
        }
        
        // Signal completion
        this.notifyStreamingListeners('', true);
        
      } else {
        console.log('üìù Non-streaming translation...');
        
        // Non-streaming fallback
        const result = await llamaContext.completion({
          prompt,
          n_predict: 64,
          temperature: 0.0,
          top_p: 0.1,
          top_k: 1,
          stop: ['<end_of_turn>', '<start_of_turn>'],
          seed: 42,
        });
        
        fullTranslation = result.text?.trim() || '';
        console.log('üìù Non-streaming result:', result);
        console.log('üìù Non-streaming text:', fullTranslation);
      }

      // Ensure we have some result
      if (!fullTranslation) {
        console.error('‚ùå No translation result obtained');
        fullTranslation = `[Translation failed - no result]`;
      }

      const translationTime = Date.now() - startTime;
      console.log(`üöÄ Translation completed in ${translationTime}ms: "${fullTranslation}"`);
      
      return fullTranslation;
      
    } catch (error) {
      console.error(`üöÄ Translation failed for ${fromLang} ‚Üí ${toLang}:`, error);
      // Signal completion even on error
      this.notifyStreamingListeners('', true);
      throw error;
    }
  }

  // Initialize Llama model
  static async initializeLlama(): Promise<any> {
    if (llamaContext || isLlamaInitializing) {
      return llamaContext;
    }

    isLlamaInitializing = true;
    notifyListeners();

    try {
      console.log('ModelManager: Initializing Llama model...');
      const modelPath = getModelPath('llama');
      
      const fileInfo = await FileSystem.getInfoAsync(modelPath);
      if (!fileInfo.exists) {
        throw new Error('Model file not found. Please download models first.');
      }

      console.log('ModelManager: Model file found, size:', Math.round(fileInfo.size / (1024 * 1024)), 'MB');

      const context = await initLlama({
        model: modelPath,
        n_ctx: 1024,
        n_threads: 4,
        use_mlock: false,  // Turn OFF memory locking - model won't be loaded into RAM
        use_mmap: true,    // Keep memory mapping - reads from disk as needed
        embedding: false,
      });

      llamaContext = context;
      console.log('ModelManager: Llama model initialized successfully (memory-mapped mode)');
      
      notifyListeners();
      return context;
    } catch (error) {
      console.error('ModelManager: Failed to initialize Llama:', error);
      throw error;
    } finally {
      isLlamaInitializing = false;
      notifyListeners();
    }
  }

  // Initialize Whisper model
  static async initializeWhisper(): Promise<any> {
    if (whisperContext || isWhisperInitializing) {
      return whisperContext;
    }

    isWhisperInitializing = true;
    notifyListeners();

    try {
      console.log('ModelManager: Initializing Whisper model...');
      const modelPath = getModelPath('whisper');

      const fileInfo = await FileSystem.getInfoAsync(modelPath);
      if (!fileInfo.exists) {
        throw new Error('Whisper model file not found. Please download models first.');
      }

      const context = await initWhisper({ filePath: modelPath });
      whisperContext = context;
      console.log('ModelManager: Whisper model initialized successfully');
      notifyListeners();
      return context;
    } catch (error) {
      console.error('ModelManager: Failed to initialize Whisper:', error);
      throw error;
    } finally {
      isWhisperInitializing = false;
      notifyListeners();
    }
  }

  // Initialize both models
  static async initializeAll(): Promise<void> {
    console.log('ModelManager: Initializing all models...');
    try {
      await Promise.all([
        ModelManager.initializeLlama(),
        ModelManager.initializeWhisper()
      ]);
      console.log('ModelManager: All models initialized successfully');
    } catch (error) {
      console.error('ModelManager: Failed to initialize models:', error);
      throw error;
    }
  }

  // Check available storage space
  static async checkStorageSpace(): Promise<{ available: number; total: number }> {
    try {
      const diskInfo = await FileSystem.getFreeDiskStorageAsync();
      return {
        available: diskInfo,
        total: diskInfo // Note: Total space detection varies by platform
      };
    } catch (error) {
      console.error('Error checking storage space:', error);
      return { available: 0, total: 0 };
    }
  }

  // Get model file sizes
  static async getModelSizes(): Promise<{ whisper: number; llama: number; total: number }> {
    try {
      const whisperInfo = await FileSystem.getInfoAsync(this.MODEL_PATHS.whisper);
      const llamaInfo = await FileSystem.getInfoAsync(this.MODEL_PATHS.llama);
      
      const whisperSize = whisperInfo.exists ? (whisperInfo.size || 0) : 0;
      const llamaSize = llamaInfo.exists ? (llamaInfo.size || 0) : 0;
      
      return {
        whisper: whisperSize,
        llama: llamaSize,
        total: whisperSize + llamaSize
      };
    } catch (error) {
      console.error('Error getting model sizes:', error);
      return { whisper: 0, llama: 0, total: 0 };
    }
  }

  // Clear models to free space
  static async clearModels(): Promise<void> {
    try {
      for (const path of Object.values(this.MODEL_PATHS)) {
        const fileInfo = await FileSystem.getInfoAsync(path);
        if (fileInfo.exists) {
          await FileSystem.deleteAsync(path);
          console.log(`Deleted model: ${path}`);
        }
      }
    } catch (error) {
      console.error('Error clearing models:', error);
      throw error;
    }
  }

  // Check if models exist
  static async checkModelsExist(): Promise<{ whisper: boolean; llama: boolean; both: boolean }> {
    try {
      const whisperInfo = await FileSystem.getInfoAsync(this.MODEL_PATHS.whisper);
      const llamaInfo = await FileSystem.getInfoAsync(this.MODEL_PATHS.llama);
      
      const whisperExists = whisperInfo.exists;
      const llamaExists = llamaInfo.exists;
      
      return {
        whisper: whisperExists,
        llama: llamaExists,
        both: whisperExists && llamaExists
      };
    } catch (error) {
      console.error('Error checking model existence:', error);
      return { whisper: false, llama: false, both: false };
    }
  }

  // Format bytes to human readable
  static formatBytes(bytes: number): string {
    if (bytes === 0) return '0 Bytes';
    const k = 1024;
    const sizes = ['Bytes', 'KB', 'MB', 'GB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
  }

  // Mock linguistic analysis for UI testing
  static async getLinguisticAnalysisWithLlama(
    originalText: string,
    fromLanguage: string,
    translatedText: string,
    toLanguage: string
  ): Promise<any> {
    // Return a promise that will be queued
    return new Promise((resolve, reject) => {
      const params = { originalText, fromLanguage, translatedText, toLanguage };
      
      // Add to queue
      analysisQueue.push({ resolve, reject, params });
      
      // Process queue if not already processing
      if (!isProcessingAnalysis) {
        this.processAnalysisQueue();
      }
    });
  }

  // Process the analysis queue one by one to prevent "Context is busy" errors
  private static async processAnalysisQueue(): Promise<void> {
    if (isProcessingAnalysis || analysisQueue.length === 0) {
      return;
    }

    isProcessingAnalysis = true;

    while (analysisQueue.length > 0) {
      const { resolve, reject, params } = analysisQueue.shift()!;
      
      try {
        const result = await this.performSingleLinguisticAnalysis(
          params.originalText,
          params.fromLanguage,
          params.translatedText,
          params.toLanguage
        );
        resolve(result);
      } catch (error) {
        reject(error);
      }
      
      // Small delay between analyses to ensure context is ready
      await new Promise(resolve => setTimeout(resolve, 100));
    }

    isProcessingAnalysis = false;
  }

  // The actual analysis implementation (moved from getLinguisticAnalysisWithLlama)
  private static async performSingleLinguisticAnalysis(
    originalText: string,
    fromLanguage: string,
    translatedText: string,
    toLanguage: string
  ): Promise<any> {
    console.log('üîç Starting real linguistic analysis...');
    console.log('üìù Input:', { originalText, fromLanguage, translatedText, toLanguage });
    console.log('üîß Llama context status:', llamaContext ? 'Available' : 'Not available');
    
    // Check if context is available, if not try to reinitialize
    if (!llamaContext) {
      console.log('‚ö†Ô∏è Llama context not available, attempting to reinitialize...');
      try {
        await this.initializeLlama();
        console.log('‚úÖ Llama context reinitialized successfully');
      } catch (error) {
        console.error('‚ùå Failed to reinitialize Llama context:', error);
        throw new Error('Llama context not initialized and failed to reinitialize');
      }
    }
    
    // Double-check context is available
    if (!llamaContext) {
      throw new Error('Llama context not initialized');
    }

    // Get language names for better prompt clarity
    const getLanguageName = (code: string) => {
      const languageNames: { [key: string]: string } = {
        'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',
        'it': 'Italian', 'pt': 'Portuguese', 'ru': 'Russian', 'ja': 'Japanese',
        'ko': 'Korean', 'zh': 'Chinese', 'ar': 'Arabic', 'hi': 'Hindi',
        'th': 'Thai', 'vi': 'Vietnamese', 'nl': 'Dutch', 'pl': 'Polish',
        'tr': 'Turkish', 'sv': 'Swedish', 'da': 'Danish', 'no': 'Norwegian'
      };
      return languageNames[code] || code.toUpperCase();
    };

    const fromLanguageName = getLanguageName(fromLanguage);
    const toLanguageName = getLanguageName(toLanguage);

    // Construct the LLM prompt for linguistic analysis focusing on source and target languages
const prompt = `<start_of_turn>user
Break down and align *smaller phrase segments* between the following sentences:

${fromLanguageName}: "${originalText}"  
${toLanguageName}: "${translatedText}"

Return strict JSON:
{
  "phrases": [
    ["${fromLanguageName}_phrase", "${toLanguageName}_phrase"]
  ]
}

Rules:
- Do not return full sentence pairs.
- Strictly match meaningful phrase segments only.
- Avoid redundant or nested phrases.
- Return only distinct segments that clearly align in meaning.
- Aim for phrase-level granularity, not sentence-level.

<end_of_turn>
<start_of_turn>model
`;



    console.log('üöÄ Sending linguistic analysis prompt...');
    
    try {
      // Verify context is still valid before making the call
      if (!llamaContext) {
        throw new Error('Llama context lost during analysis preparation');
      }
      
      const result = await llamaContext.completion({
        prompt,
        n_predict: 300, // Further increase token limit to avoid truncation
        temperature: 0.0,
        top_p: 0.1,
        top_k: 1,
        stop: ['<end_of_turn>', '<start_of_turn>'],
        repeatpenalty: 1.2, // Adjusted to avoid repetition
        seed: 42,
      });

      console.log('üì• Raw LLM response:', result.text);
      
      // Verify context is still valid after the call
      if (!llamaContext) {
        console.warn('‚ö†Ô∏è Llama context lost after completion call');
      }
      
      // Parse the JSON response
      let jsonText = result.text?.trim() || '';
      
      // Check if response was truncated
      if (!jsonText.includes('}') || jsonText.endsWith('part')) {
        console.error('‚ö†Ô∏è Response appears to be truncated:', jsonText.slice(-50));
        throw new Error('LLM response was truncated. Please try again.');
      }
      
      // Remove markdown code fences if present
      jsonText = jsonText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
      jsonText = jsonText.replace(/^```\s*/, '').replace(/\s*```$/, '');
      
      console.log('üîß Cleaned JSON text:', jsonText);
      
      // Additional validation - ensure JSON ends properly
      if (!jsonText.trim().endsWith('}')) {
        console.error('‚ö†Ô∏è JSON does not end properly:', jsonText.slice(-100));
        throw new Error('Incomplete JSON response. Please try again.');
      }
      
      const analysis = JSON.parse(jsonText);
      console.log('‚úÖ Parsed analysis:', analysis);
      
      // Validate that each phrase is an array with 2 elements
      if (analysis.phrases) {
        analysis.phrases.forEach((phrase: any, index: number) => {
          if (Array.isArray(phrase) && phrase.length === 2) {
            console.log(`üîç Phrase ${index}:`, {
              fromLanguagePhrase: phrase[0],
              toLanguagePhrase: phrase[1],
            });
          } else {
            console.warn(`‚ö†Ô∏è Phrase ${index} invalid format (expected 2 elements):`, phrase);
          }
        });
      }
      
      console.log('üîß Final context status:', llamaContext ? 'Still available' : 'Lost');
      
      return analysis;
      
    } catch (error) {
      console.error('‚ùå Linguistic analysis failed:', error);
      console.log('üîß Context status after error:', llamaContext ? 'Still available' : 'Lost');
      throw new Error(`Failed to analyze sentence structure: ${error}`);
    }
  }
}